import os
import json
import requests
from dotenv import load_dotenv, find_dotenv
try:
    from openai import OpenAI
except Exception:
    OpenAI = None

def _mask(s):
    if not s:
        return ""
    return s[:4] + "***" + s[-4:]

def call_llm(system: str, user: str, streaming: bool = False) -> str:
    load_dotenv(find_dotenv())
    provider = os.environ.get("LLM_PROVIDER", "deepseek")
    base_url = os.environ.get("LLM_BASE_URL", os.environ.get("DEEPSEEK_BASE_URL", "https://api.deepseek.com"))
    api_key = os.environ.get("LLM_API_KEY", os.environ.get("DEEPSEEK_API_KEY", ""))
    model = os.environ.get("LLM_MODEL", os.environ.get("DEEPSEEK_MODEL", "deepseek-reasoner"))
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
        "stream": False,
    }
    try:
        if not api_key:
            print("LLM未配置: 缺少 LLM_API_KEY")
            return ""
        if OpenAI is not None:
            try:
                client = OpenAI(api_key=api_key, base_url=base_url)
                resp = client.chat.completions.create(model=model, messages=payload["messages"], stream=False)
                content = resp.choices[0].message.content if resp and resp.choices else ""
                return content or ""
            except Exception as e:
                print(f"LLM(OpenAI SDK)异常: {e}")
        resp = requests.post(f"{base_url}/chat/completions", headers=headers, data=json.dumps(payload), timeout=30)
        if not resp.ok:
            print(f"LLM调用失败: {resp.status_code} {resp.text[:200]}")
            return ""
        data = resp.json()
        if "choices" in data and data["choices"]:
            return data["choices"][0]["message"]["content"]
        print("LLM返回空choices")
        return ""
    except Exception as e:
        print(f"LLM异常: {e}")
        return ""
